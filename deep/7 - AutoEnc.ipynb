{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 - Deep Learning\n",
    "\n",
    "Обучите автоэнкодер на датасете `MNIST`, постарайтесь сделать его с размером эмбеддинга не больше 128. Сгенерируйте эмбеддинги для тысячи объектов из обучающей выборки (как в семинаре), обучите на этих эмбеддингах **Случайный лес**. Добейтесь **Accuracy** больше **90%**.\n",
    "\n",
    "Сделайте эмбеддинги для 1000 объектов из обучающей выборки, тензор с ними должен иметь размерность (1000,*N*), где *N*≤128. Мы обучим на этом тензоре Случайный лес из библиотеки `scikit-learn`, а именно `RandomForestClassifier(random_state=0)`. После чего проверим качество на ваших эмбеддингах для тестовой выборки, сдайте их тоже. Тензор с эмбеддингами для тестовой выборки должен иметь размерность (10000,*N*). Accuracy нашего обученного случайного леса должна быть больше 90% на тестовых эмбеддингах.\n",
    "\n",
    "Чтобы корректно сдать все эмбеддинги воспользуйтесь этой функцией:\n",
    "\n",
    "```python\n",
    "def save_embeddings(x_train, y_train, x_valid, y_valid):\n",
    "    assert x_train.shape[0] == 1000\n",
    "    assert x_valid.shape[0] == 10000\n",
    "    \n",
    "    assert y_train.shape[0] == 1000\n",
    "    assert y_valid.shape[0] == 10000\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            'x_train': x_train,\n",
    "            'y_train': y_train,\n",
    "            'x_valid': x_valid,\n",
    "            'y_valid': y_valid\n",
    "        },\n",
    "        'embeddings.pt'\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from IPython.display import clear_output\n",
    "from torchvision.datasets import MNIST\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from time import perf_counter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import alexnet, vgg11, googlenet, resnet18\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\n",
    "from typing import List, Tuple\n",
    "\n",
    "# import importlib\n",
    "# task5 = importlib.import_module(\"5 - Architects\")\n",
    "# task4 = importlib.import_module(\"4 - RegularizeConvNet\")\n",
    "# task3 = importlib.import_module(\"3 - ConvNets\")\n",
    "# task2 = importlib.import_module(\"2 - NeuralOptim\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rc('font', size=20)\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 17043570.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz to mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 5740212.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz to mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 5899431.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to mnist\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_transforms = T.Compose(\n",
    "    [\n",
    "        T.Resize((64, 64)),\n",
    "        T.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = MNIST('mnist', train=True, transform=mnist_transforms, download=True)\n",
    "valid_dataset = MNIST('mnist', train=False, transform=mnist_transforms, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
